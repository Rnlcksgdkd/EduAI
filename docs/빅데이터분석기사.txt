빅데이터분석기사 (2024년 개정) 이론 및 개념 정리

빅데이터분석기사 자격은 방대한 데이터를 이해하고 수집·저장·처리·분석·시각화할 수 있는 빅데이터 분석 전문가의 역량을 검증하는 국가기술자격입니다​
2024년 개정 기준으로 필기시험은 데이터 이해부터 시각화까지 전 범위의 이론과 개념을 다루며, 각 과목별로 핵심 내용을 정리하면 다음과 같습니다.

데이터 이해 (Big Data Understanding)
빅데이터의 개념: 빅데이터란 기존의 데이터베이스 관리 도구로 처리하기 어려울 만큼 막대한 규모의 정형 및 비정형 데이터로부터 가치를 추출하고 결과를 분석하는 기술을 말합니다​. 
간단히 말해 다양한 형태의 대규모 데이터를 효과적으로 처리하여 유용한 정보를 얻는 것을 의미합니다. 빅데이터는 기존의 소규모 데이터 분석과 달리 병렬 처리 기반의 분산 시스템이 필요하며, 장기적인 가치 창출에 초점을 맞춘다는 차이가 있습니다. 
빅데이터의 특징 (3V, 5V): 빅데이터의 대표적인 특성으로 3V를 꼽습니다. 규모(Volume), 다양성(Variety), **속도(Velocity)**를 말하며, 데이터 양이 방대하고 종류가 다양하며 생성 및 처리 속도가 빠른 데이터를 다룬다는 뜻입니다​. 
이후 여기에 **정확성(Veracity)**과 가치(Value) 요소가 추가되어 5V 특성으로 확장되었습니다​.
정확성은 데이터의 신뢰성과 타당성을 확보하는 특성으로, 노이즈나 오류를 걸러내어 품질 높은 데이터만을 분석에 사용함을 의미합니다. 가치는 빅데이터 분석의 최종 목표로, 대량 데이터로부터 유용한 인사이트와 비즈니스 가치를 창출해야 빅데이터를 활용하는 의미가 있습니다. (최근 일부에서는 7V로 확대하여 정확성(Validity), 휘발성(Volatility) 등을 추가하기도 합니다.) 데이터의 가치와 산업: 전 세계적으로 빅데이터는 미래 사회의 핵심 성장동력으로 인식되고 있으며, 각국과 기업들은 데이터 분석을 주요 전략으로 채택하고 있습니다​.
방대한 데이터를 기반으로 한 의사결정은 경쟁력을 높이기 때문에, 데이터 경제와 데이터 산업이 급격히 성장하고 있습니다. 예를 들어 각 산업 분야(과학기술, 의료, 제조, 금융, 교통 등)에서 데이터 기반 혁신이 이루어지고 있고, 새로운 비즈니스 모델(추천 시스템, 개인화 마케팅, 스마트 시티 등)이 데이터 분석을 통해 창출되고 있습니다. 이러한 흐름 속에서 빅데이터 전문인력에 대한 수요도 크게 늘어나고 있으며, 정부와 기업 차원에서 데이터 전문가 양성에 힘쓰고 있습니다. 빅데이터 조직과 인력: 효과적인 빅데이터 활용을 위해서는 전문적인 조직 구성과 인력이 필요합니다. 보통 데이터 엔지니어, 데이터 분석가, 데이터 과학자 세 가지 직무가 핵심 역할을 맡습니다​.
데이터 엔지니어는 데이터의 수집, 저장, 유지·보수 등 데이터 관리를 담당하는 역할로, 조직 내 필요한 데이터를 잘 정제되고 완전한 형태로 신속히 제공하는 것이 목표입니다​
이들은 하둡, 분산 파일시스템, 관계형 데이터베이스(SQL) 등 데이터 인프라에 대한 전문 지식을 갖추고 있습니다. 
데이터 분석가는 말 그대로 주어진 데이터를 분석하여 인사이트를 도출하는 사람들로, 비즈니스 목적에 맞게 데이터를 탐색하고 패턴을 찾아내어 조직의 의사결정에 활용할 수 있는 결과를 만들어냅니다​. 
주로 과거와 현재의 데이터를 대상으로 서술적 분석(descriptive analysis)을 수행하며, 통계적 지식과 R, 파이썬 같은 분석 도구 활용 능력, 그리고 결과를 이해하기 쉽게 시각화하는 능력이 중요합니다​. 
데이터 과학자는 데이터 분석가의 역할에 더해 머신러닝/딥러닝 등의 고급 분석기법을 활용하여 미래를 예측하는 데 초점을 둔 직무입니다​. 
즉 예측적 분석(predictive analysis)의 비중이 크며, 대규모 데이터로부터 복잡한 패턴을 학습해 향후 발생 가능한 결과를 모델링합니다. 
데이터 과학자는 통계와 프로그래밍 능력은 물론 머신러닝 알고리즘과 모델링에 대한 깊은 이해를 갖추고, 실제로 모델을 구축(모델링)하고 평가하여 조직에 최적의 예측 모델을 제시하는 역할을 합니다. 
빅데이터 기술 플랫폼: 빅데이터를 저장하고 처리하기 위해서는 기존의 RDBMS로는 한계가 있으므로 분산처리 플랫폼을 활용합니다. 
대표적으로 **하둡(Hadoop)**과 스파크(Spark) 등이 있습니다. 하둡은 대용량 데이터를 여러 대의 서버에 분산 저장하고 병렬 처리하는 자바 기반 오픈소스 프레임워크로, 분산 파일 시스템 HDFS와 맵리듀스(MapReduce) 엔진을 기반으로 동작합니다​. 
하둡은 한꺼번에 일괄(batched)로 처리하는 작업에 강점이 있으며, 엄청난 양의 데이터를 블록 단위로 나누어 분산 저장하고 병렬 연산을 수행함으로써 처리량을 높입니다. 스파크는 하둡 이후 등장한 기술로, 메모리 기반의 분산처리 엔진입니다. 
하둡의 맵리듀스에 비해 실시간 데이터 처리와 반복 연산에 최적화되어 있는데, 메모리에서 연산함으로써 반복 작업에서 하둡보다 최대 1000배 이상 빠른 성능을 보입니다​. 
오늘날 빅데이터 플랫폼은 보통 하둡의 분산 저장 능력과 스파크의 고속 처리 능력을 연계하여 활용하며, 이 외에도 NoSQL 데이터베이스(예: MongoDB, Cassandra)나 데이터 웨어하우스, 클라우드 빅데이터 서비스 등이 빅데이터 기술 스택을 구성합니다. 이러한 플랫폼을 통해 정형 데이터부터 비정형 데이터까지 효율적으로 저장하고 분석할 수 있습니다. 빅데이터와 인공지능: 빅데이터 기술의 발전은 인공지능(AI) 특히 머신러닝/딥러닝의 발전과 밀접한 관련이 있습니다. 한편으로 빅데이터는 인공지능 모델의 학습용 데이터를 풍부하게 제공하여 정밀한 예측을 가능케 하고, 다른 한편으로 인공지능 기술은 빅데이터에 담긴 유의미한 패턴을 자동으로 찾아내는 도구로 활용됩니다. 예를 들어 대량의 이미지, 음성, 텍스트 데이터를 딥러닝 모델로 학습시켜 높은 성능의 AI를 구현하는 것이 가능해졌습니다. 즉, 빅데이터가 연료(data is fuel)라면 머신러닝 알고리즘은 엔진에 비유될 수 있으며, 둘의 결합이 4차 산업혁명 시대의 핵심 기술 트렌드를 이루고 있습니다. 개인정보 보호 법·제도: 빅데이터 분석에서는 데이터 윤리와 개인정보 보호도 중요한 이슈입니다. 대량의 데이터에 개인에 관한 정보가 포함되는 경우가 많아 프라이버시 침해 우려가 있기 때문에, 각국에서는 개인정보 보호법 등으로 데이터 활용에 일정한 제약과 가이드라인을 두고 있습니다. 우리나라의 경우 개인정보보호법 및 관련 규정에 따라, 개인을 알아볼 수 없도록 처리한 가명정보는 신용정보 분야나 통계·연구 등의 목적으로 동의 없이 활용할 수 있도록 2020년 법 개정으로 제도화하였습니다. 이는 데이터 비식별화(De-identification)를 통해 개인정보 침해를 최소화하면서 데이터의 활용 가치를 살리기 위한 방안입니다. 데이터 비식별화 방법으로는 가명처리(예: 이름을 다른 식별자로 대체)나 총계처리(개별 데이터 값을 삭제하고 집계값만 활용) 등이 있으며​
, 이러한 조치를 거친 가명정보를 분석에 활용하면 법적으로 개인정보가 아닌 것으로 간주되어 보다 폭넓은 활용이 가능합니다. 더불어 빅데이터 시대에는 동의 기반의 활용에서 책임 기반의 활용으로 패러다임 전환도 논의됩니다. 즉, 일일이 이용자 동의를 받기 어려운 방대한 데이터 활용에서는 데이터 활용자가 책임을 지고 개인정보를 보호하는 체계로의 변화가 필요하다는 것입니다​.
이처럼 법·제도적으로 개인정보 보호를 강화하면서도 안전한 활용을 도모하기 위해 프라이버시 보호 기술(예: 데이터 마스킹, 차등정보보호 등)과 거버넌스 체계가 정비되고 있습니다. 
빅데이터 분석 기획: 빅데이터 프로젝트를 수행할 때는 체계적인 **분석 기획(로드맵)**이 필요합니다. 먼저 해결하려는 문제 정의를 명확히 해야 합니다. 조직의 비즈니스 목표나 현안으로부터 분석 과제를 도출하고, 어떤 의사결정에 기여할 것인지 목표를 설정합니다. 다음으로 목표를 달성하기 위한 분석 방안을 수립합니다. 이는 어떤 데이터를 활용하고 어떤 기법으로 분석할지 큰 그림을 그리는 단계입니다. 예를 들어 *“고객 이탈을 예측하고 방지 전략 수립”*이라는 문제라면, 고객 거래 데이터와 상담 기록 등의 데이터 확보 방안을 세우고, 이를 기반으로 머신러닝 분류 모델을 활용하는 식의 분석 방법을 계획할 수 있습니다. 이렇게 분석 로드맵을 설정한 뒤에는 분석 절차 및 작업 계획을 구체화합니다. 프로젝트 일정, 필요 인력과 역할, 데이터 수집 일정, 모델링 및 평가 일정 등을 작성하여 언제 어떤 순서로 분석을 진행할지 계획합니다. 이 단계에서는 국제적으로 통용되는 데이터 분석 방법론인 CRISP-DM 등을 참고하여, 비즈니스 이해 → 데이터 이해 → 데이터 준비 → 모델링 → 평가 → 전개 등의 과정을 로드맵에 반영할 수도 있습니다. 
요약하면, 데이터 분석 기획 단계에서는 무엇을, 왜 분석하고, 어떤 데이터와 방법으로, 어떻게 추진할 것인지를 미리 설계함으로써 성공적인 프로젝트 수행을 준비합니다.

데이터 수집 (Data Collection)
데이터 수집 개요: 계획 단계에서 확보하기로 한 데이터를 실제로 모으는 단계입니다. 데이터 수집이란 말 그대로 다양한 원천으로부터 목적에 맞는 데이터를 추출하고 획득하는 작업입니다. 빅데이터 분석에서는 내부 데이터뿐 아니라 외부 데이터까지 폭넓게 활용되므로, 내부 데이터베이스, 로그, ERP 등에서 추출하는 사내 데이터 수집과, 공공데이터, 웹 데이터, 소셜미디어, 센서 등에서 가져오는 외부 데이터 수집을 모두 고려합니다. 예를 들어 금융사의 빅데이터 분석이라면, 사내의 거래 데이터와 고객정보 외에 소셜미디어의 여론 데이터나 외부의 경제 지표 데이터를 함께 수집하여 통합적으로 분석할 수 있습니다. 데이터 유형과 속성 파악: 수집 단계에서는 다루는 데이터의 유형을 이해하는 것이 중요합니다. 
데이터는 크게 정형(structured), 반정형(semi-structured), 비정형(unstructured) 데이터로 구분됩니다​. 
정형 데이터는 컬럼과 스키마가 정해진 표 형태의 데이터로, 전통적 관계형 데이터베이스나 스프레드시트(CSV 등)로 저장되는 형태입니다. 
숫자나 코드로 이루어져 있어 구조가 체계적이며, 비교적 쉽게 의미를 해석할 수 있습니다​. 
비정형 데이터는 사전 정의된 구조가 없는 데이터로, 텍스트 문서, 로그 파일, 이미지, 영상, 오디오, SNS 메시지 등이 해당됩니다​. 
비정형 데이터는 규칙이 일정하지 않아 바로 연산하거나 DB에 저장하기 어렵지만, 텍스트 마이닝이나 이미지 분석 등의 기법을 통해 의미를 추출함으로써 새로운 인사이트를 얻을 수 있습니다 (예: 소셜 여론 분석으로 소비자 반응 파악). 
반정형 데이터는 정형과 비정형의 중간 성격으로, 일정한 형식은 갖고 있으나 완전한 스키마를 가지지 않는 데이터를 말합니다​. 
예를 들어 JSON, XML, HTML 데이터나 시스템 로그 등이 반정형 데이터에 속합니다. 태그나 구분자가 있어 부분적으로 구조화돼 있지만 필드 내용은 자유로운 텍스트인 경우가 많아, 이러한 데이터를 다룰 때는 **파싱(parsing)**을 통해 유의미한 구조를 추출해야 합니다. 이처럼 수집하려는 데이터의 유형을 파악하면, 적절한 저장방식(RDB vs NoSQL 등)과 처리기술(예: 텍스트 처리기술)을 선택할 수 있습니다. 또한 데이터 수집 시에는 데이터의 속성들도 확인합니다. 데이터 발생 주기(실시간 스트리밍 데이터인지, 일별/월별 집계인지), 크기(GB, TB 단위의 용량), 형식(숫자, 문자열, 멀티미디어), 품질 상태(오류나 중복 여부) 등을 미리 파악해야 이후 처리 방향을 정할 수 있습니다. 예를 들어 센서 데이터나 거래 로그처럼 실시간 발생하는 데이터는 수집 단계부터 스트리밍 처리나 적재에 신속성이 요구되며, 정기 집계 데이터는 배치 수집으로도 충분합니다. 데이터 수집 방법: 수집 경로에 따라 다양한 기술과 방법론이 활용됩니다. 정형 데이터의 경우 기존 데이터베이스나 시스템에서 필요한 데이터를 **추출(Extract)→변환(Transform)→적재(Load)**하는 ETL 방식이 일반적입니다. 예컨대 기업의 DW(데이터웨어하우스)에 쌓인 데이터를 추출하여 분석 서버로 옮기는 작업이 이에 해당합니다. API를 통해 데이터를 조회하여 가져오거나, 파일로 일괄 받는 방식도 있습니다​.
비정형 데이터 수집을 위해서는 **웹 크롤링(web crawling)**이나 스크레이핑(scraping) 기술이 많이 사용됩니다. 
이는 인터넷 웹페이지에서 필요한 텍스트나 이미지를 자동으로 수집하는 방법으로, 파이썬의 BeautifulSoup 라이브러리나 Scrapy 프레임워크 등이 활용됩니다​. 
소셜 미디어나 Open API로 제공되는 데이터는 Open API 호출을 통해 실시간으로 수집하기도 합니다. 스트리밍 데이터의 경우 Apache Kafka와 같은 분산 메시지 큐 시스템을 사용하여 실시간으로 데이터를 파이프라인에 흘려보낼 수 있습니다​. 
센서 데이터나 IoT 데이터는 센싱 장치에서 생성되어 네트워크를 통해 실시간으로 전송되며, 이를 수집하는 스트리밍 처리 시스템(예: Flume, MQTT 등)이 동원됩니다​. 
반정형 데이터(예: 로그, JSON)는 수집 후에 데이터 변환 작업이 수반됩니다. 로그 수집 도구나 파서(parser)를 이용해 필요한 필드를 뽑아내 정형화하거나, JSON/XML 데이터를 계층 구조에서 테이블 형태로 변환하여 저장합니다. 데이터 적재와 저장: 수집한 데이터를 분석에 활용하려면 적절한 **저장소(storage)**에 넣어 관리해야 합니다. 데이터의 특성과 규모에 따라 저장 방식이 달라집니다. 
정형 데이터는 보통 **관계형 데이터베이스(RDB)**나 데이터 웨어하우스에 적재됩니다. 예컨대 MySQL, Oracle DB에 테이블 형태로 저장하거나, 대규모 정형 데이터는 하둡 HDFS 위에 Hive 테이블로 저장하여 분산 SQL 조회가 가능하게 합니다. 비정형 데이터는 파일 시스템이나 NoSQL 데이터베이스에 저장하는데, 대용량 텍스트/이미지라면 하둡 HDFS에 파일로 저장하거나, MongoDB같은 문서 지향 NoSQL DB에 JSON 형태로 저장할 수 있습니다. 또한 오늘날에는 데이터 레이크(Data Lake) 개념이 도입되어, 정형/비정형 구분 없이 모든 raw 데이터를 한 곳에 모아두고 필요에 따라 가공하여 쓰는 전략을 쓰기도 합니다. 중요 한점은, 수집 단계에서 데이터가 손실되지 않고 원시 상태로 잘 저장되어야 이후 단계에서 자유롭게 가공하고 분석할 수 있다는 것입니다. 데이터 적재 시에는 ETL 파이프라인이 자동화되어야 하며, 스키마가 있는 데이터의 경우 스키마 정합성을 맞추고 인덱스 등의 성능 요소도 고려합니다. 데이터 품질 검증: 데이터를 수집한 후 분석에 활용하기 전에, 데이터 품질을 점검해야 합니다. 품질 높은 데이터란 정확하고 일관성 있으며 결측이나 오류가 최소화된 데이터를 말합니다. 수집 단계에서 흔히 발생하는 문제로는 중복 데이터, 오타나 잘못된 값, 자료 누락, 불일치 등이 있습니다. 예를 들어 동일한 고객이 두 번 등록되어 중복되었거나, 센서 오작동으로 비현실적인 값(온도 -100도 등)이 들어온 경우 등입니다. 이러한 문제를 발견하기 위해 기술통계 요약(건수, 평균, 분산 등)을 통해 값의 분포를 확인하고, 예상 범위를 벗어나는 이상치나 결측치(missing value) 발생 상황을 파악합니다. 또 데이터 스키마와 메타데이터를 검사하여, 모든 필드가 올바르게 채워져 있는지, 날짜 형식이나 단위 등이 일관적인지 확인합니다. 데이터 품질 검증은 이후 데이터 정제 단계에서 무엇을 손볼지 결정하는 데 중요한 입력이 됩니다. 품질 문제가 심각한 데이터 소스의 경우 분석에서 제외하거나 추가 수집을 통해 보완할 수도 있습니다. 데이터 비식별화: 만약 수집된 데이터에 개인정보나 민감정보가 포함되어 있다면, 본격 분석 전에 비식별화 조치를 해야 합니다(법적 요구 사항 준수 및 윤리적 사용). 예를 들어 고객ID, 이름, 전화번호 등이 있다면, 이를 해싱 또는 치환하여 직접적으로 개인을 식별할 수 없도록 합니다. 앞서 언급한 가명처리가 대표적이며, 추가로 데이터 범주화(나이 → 연령대 구간으로 변환)나 값 삭제(민감한 필드 제거) 같은 방법도 사용됩니다​.
이러한 비식별 데이터는 분석 목적상 크게 지장이 없는 수준에서 수행하는 것이 바람직합니다. 비식별화가 너무 심하면 데이터의 유용성까지 떨어질 수 있으므로, 균형 있게 개인정보를 보호하면서 데이터 활용성을 유지하는 것이 중요합니다.


데이터 처리 (Data Processing)
데이터 전처리의 중요성: 수집한 원시(raw) 데이터는 그대로 분석에 활용하기 어려운 경우가 많습니다. 오류나 잡음이 섞여 있거나, 형식이 일관되지 않고, 분석 알고리즘에 맞지 않는 형태일 수 있기 때문입니다. 
데이터 처리 단계에서는 본격적인 분석에 앞서 데이터를 **정제(cleaning)**하고 **가공(transforming)**하여 분석에 적합한 상태로 만드는 과정을 거칩니다. 이 단계를 흔히 **데이터 전처리(data preprocessing)**라고 부르며, 
전체 분석 과정 중 상당한 시간과 노력을 차지하지만 결과의 신뢰도를 높이기 위해 필수적인 작업입니다. 데이터 정제 (Cleaning): 우선 데이터셋에 존재하는 잘못된 부분을 정제합니다. 정제 작업에는 여러 종류가 있습니다. 
대표적으로 결측값(missing value) 처리와 이상값(outlier) 처리가 있습니다. 결측값이란 값이 비어 있는 경우로, 설문 무응답, 센서 통신 오류 등으로 인해 발생합니다. 
결측 데이터가 많으면 분석 결과가 왜곡될 수 있으므로, 삭제 또는 대체로 처리합니다. 결측 레코드가 소수라면 해당 행(row)을 제거하거나, 중요한 변수의 결측이라면 해당 사례를 제외할 수 있습니다. 
그렇지 않으면 대체(imputation) 기법을 사용해 빈 값을 채웁니다. 평균이나 중앙값으로 채우는 단순대체, 인접한 시계열 값으로 채우는 보간, 혹은 예측 모델을 사용한 다중대체 방법도 있습니다. 
어떤 방식을 쓰든 자료의 분포를 크게 훼손하지 않도록 신중히 적용해야 합니다. 이상값은 정상적 범위를 벗어난 극단적인 값으로, 오류이거나 특이한 패턴일 수 있습니다. 예를 들어 사람 키 데이터에 300cm가 있다면 입력 오류로 볼 수 있습니다. 
이상치를 다루는 방법으로는 필터링/삭제가 있으며, 사전에 정의한 합리적 범위를 벗어나는 값은 제거하거나 별도 저장합니다. 
또는 이상치가 극단값이지만 의미가 있다고 판단되면 값을 한계점으로 Winsorizing하거나 로그 변환 등으로 영향력을 완화하기도 합니다. 
이 외에도 데이터 정제에는 오타 수정, 단위 통일 (예: kg과 lb 혼용시 하나로 통일) 등이 포함됩니다. 정제 단계를 통해 데이터의 정확성과 일관성을 확보하게 됩니다. 
변수(feature) 처리: 분석에 사용할 **변수(피처)**들을 다듬는 일도 중요합니다. 원본 데이터에는 분석에 불필요한 변수도 있고, 반대로 여러 변수를 조합해 새로운 정보를 담은 파생변수를 만들어낼 수도 있습니다. 
**변수 선택(feature selection)**은 모델 성능에 크게 기여하지 않는 변수나 중복된 변수를 제거하여 분석 효율과 모델 일반화 성능을 높이는 작업입니다. 
너무 많은 변수가 있으면 차원의 저주 문제가 발생할 수 있으므로, 전문가의 도메인 지식이나 통계 기법(예: 상관분석, 카이제곱 검정, 정보획득량 등)을 통해 주요 변수를 선별합니다. 
예를 들어 고객 이탈 예측에서 고객 ID나 주소 등은 예측에 영향이 적으므로 제외할 수 있습니다. 한편, 차원축소(dimensionality reduction) 기법을 활용하기도 합니다. 
차원축소는 여러 변수가 내포한 정보를 최대한 유지하면서 변수 개수를 줄이는 기법입니다. 대표적으로 **주성분 분석(PCA)**이 있습니다. 
PCA는 다수의 상관된 변수들을 선형 결합하여 상호 독립적인 주성분으로 변환함으로써, 원래 데이터의 분산을 대부분 설명하는 몇 개의 축으로 데이터를 축약합니다. 
이를 통해 노이즈나 상관관계로 인한 불필요 차원을 제거하고 모델의 복잡성을 줄일 수 있습니다. 그 밖에 요인분석, 다차원척도법 등도 차원축소 기법에 포함됩니다. 
다만 차원축소를 쓰면 해석력이 떨어질 수 있으므로 (변환된 축은 본래 의미와 동떨어질 수 있음) 적용 여부는 분석 목적에 따라 결정합니다. 또한 **파생변수 생성(feature engineering)**도 전처리의 핵심입니다. 
기존 변수들을 조합하거나 수식 변환하여 의미 있는 새로운 변수를 만들어내는 작업입니다. 예를 들어 총구매액 변수를 고객별 구매횟수와 평균객단가로부터 계산해내거나, BMI지수를 키와 체중 변수로부터 만들어 건강상태 지표로 사용하는 식입니다. 
잘 설계된 파생변수는 모델이 복잡한 패턴을 더 쉽게 잡아낼 수 있도록 도와주어 예측력을 향상시킵니다. 변수 변환 및 인코딩: 분석 기법에 적합하도록 데이터 형태를 변환하는 작업도 필요합니다. 
대표적으로 **스케일링(scaling)**과 **인코딩(encoding)**이 있습니다. 스케일링은 변수의 값 범위를 조정하는 것으로, **정규화(normalization)**와 표준화(standardization) 방법이 있습니다. 
정규화는 모든 값을 0~1 사이로 변환(최소-최대 정규화)하여 특징 규모를 맞추는 것이고, 표준화는 평균을 0, 표준편차를 1로 변환하여 정규분포 형태로 만드는 것입니다. 
스케일링은 거리 기반 모델(KNN, 군집)이나 최적화 기반 모델(신경망, SVM)에서 변수 간 단위 차이에 따른 영향 불균형을 막기 위해 필수적으로 적용됩니다. 
인코딩은 범주형(categorical) 데이터를 수치형으로 변환하는 작업입니다. 예를 들어 남/녀, 제품 등급 A/B/C 같은 범주 데이터를 **원-핫 인코딩(one-hot encoding)**이나 라벨 인코딩 등을 통해 모델에 입력할 수 있는 숫자로 표현합니다. 
원-핫 인코딩은 범주별 이진(dummy) 변수를 만드는 것이고, 라벨 인코딩은 범주에 일련번호를 부여하는 방식인데, 후자는 순서나 크기 의미가 없을 때는 잘 쓰지 않습니다. 
그 외에 로그 변환(오른쪽으로 긴 분포를 정규화하기 위해 log를 취함), 박스콕스 변환(분포 정규화), 다항식 변환(비선형 관계를 포착하기 위해 등 분석 기법에 맞춰 변수를 변환하기도 합니다. 
불균형 데이터 처리: 지도학습 문제에서 클래스 불균형이 있는 경우 추가 처리도 고려해야 합니다. 
예를 들어 10000개 데이터 중 1%만 긍정 클래스이고 99%가 부정 클래스인 경우, 모델이 모두 부정으로만 예측해도 99% 정확도가 나와버리는 문제가 있습니다. 
이러한 imbalanced data 상황에서는 소수 클래스의 데이터를 증강하거나 다수 클래스를 일부 사용하지 않는 등 균형을 맞출 수 있습니다. 
오버샘플링(over-sampling) 기법으로는 소수 클래스 데이터를 복제하거나 노이즈를 약간 줘서 늘리는 방법, 또는 더 세련되게 SMOTE 기법을 사용해 소수 클래스 데이터 포인트들 사이를 보간하여 새로운 데이터를 생성하는 방법이 있습니다. 
**언더샘플링(under-sampling)**은 다수 클래스의 일부 데이터를 무작위 제거하여 균형을 맞추는 방식입니다. 둘 다 데이터 분포를 인위적으로 바꾸는 것이므로 주의가 필요하며, 경우에 따라 가중치 조정(모델 학습 시 소수 클래스 오류에 더 큰 패널티 부여)으로 대응하기도 합니다. 
결국 목표는 모델이 희귀한 중요한 사례도 놓치지 않도록 학습 데이터를 준비하는 것입니다. 전처리 과정을 거친 결과, 원시 데이터는 깨끗하고 일관된 형태의 분석용 데이터셋으로 변환됩니다. 이 데이터셋은 이제 통계 분석이나 머신러닝 모델에 투입할 수 있는 상태가 되며, 전처리의 품질이 최종 분석 성능을 좌우할 만큼 중요합니다.


데이터 분석 (Data Analysis)
데이터 탐색 (Exploratory Data Analysis): 본격적인 모델링이나 통계 분석을 수행하기 전에, 우선 데이터를 이해하기 위한 탐색(EDA) 단계를 거칩니다. 데이터 탐색은 데이터의 분포와 관계, 특이점 등을 파악하여 이후 적용할 분석 기법의 방향을 잡고 가설을 수립하는 과정입니다. 주요 방법으로 기초 통계량 산출과 시각화 기초 분석, 상관관계 분석 등이 있습니다.
기초 통계량 추출: 각 변수의 분포와 대표값을 살펴봅니다. 평균, 중앙값, 최빈값, 표준편차, 사분위수(1Q, 3Q) 등의 **기술통계(descriptive statistics)**를 계산하여 데이터의 중심 경향과 산포도를 파악합니다. 
이를 통해 값의 범위와 단위, 자료의 치우침(skewness) 여부 등을 알 수 있습니다. 예를 들어 소득 데이터라면 평균 대비 중앙값이 낮으면 우측으로 긴 분포(고소득자 소수)임을 알 수 있습니다. 또한 변수 간 **상관관계(correlation)**를 계산하여 (주로 피어슨 상관계수 사용) 어떤 변수들이 강한 양의 상관, 음의 상관이 있는지 확인합니다. 상관관계 분석을 통해 다중공선성 문제를 예측하거나 원인-결과 관계의 단서를 얻을 수 있습니다​.
다만 상관관계는 인과관계를 의미하지 않음을 염두에 두고 해석해야 합니다.
시각적 탐색: 수치 요약과 함께 그래프를 통한 탐색이 효과적입니다. **히스토그램(histogram)**을 그려 분포 모양을 확인하고, **상자 그림(box plot)**으로 이상치 및 사분위 범위를 시각화합니다. 두 변수 간 관계는 **산점도(scatter plot)**로 그려 상관성이나 패턴(선형, 비선형 관계 등)을 관찰합니다. 변수 간 분포 차이는 밀도곡선이나 빈도 막대그래프로 비교하고, 범주형 데이터는 막대그래프나 모자이크 플롯 등으로 분포를 확인합니다. 이러한 시각화는 숫자만으로 파악하기 어려운 분포의 형태, 극단치 존재, 그룹 간 격차 등을 한눈에 보여주어, 데이터에 내재된 특성을 직관적으로 이해하는데 도움을 줍니다. 예를 들어 산점도를 통해 두 변수 사이에 비선형 U자형 관계가 있음을 알았다면, 선형 회귀보다는 다항 회귀를 고려하는 식으로 분석 기법 선택에 시사점을 얻을 수 있습니다.
고급 데이터 탐색: 필요에 따라 특수한 관점에서 탐색을 하기도 합니다. 시계열 데이터나 공간 데이터의 경우 시공간적 탐색을 합니다. 시간에 따른 변화 추이를 보려면 시계열 플롯(라인 차트)을 그리고, 계절성이나 추세를 파악합니다. 예컨대 일별 판매량 데이터를 시계열로 그리면 주말 상승 패턴이나 연말 증가 추세를 확인할 수 있습니다. 공간적 탐색은 지리적 데이터에 대해 지도 시각화(예: 지역별 색깔로 수치 표현하는 지도) 등을 통해 지역 간 분포 차이를 봅니다. 다변량 데이터 탐색은 세 개 이상 변수의 복합 관계를 살피는 것으로, **산점도 행렬(scatter matrix)**이나 평행좌표 그래프 등을 사용합니다. 산점도 행렬은 여러 변수 쌍의 산점도를 매트릭스로 표시하여 한꺼번에 상관 관계를 시각화하고, 평행좌표 그래프는 개별 관측치의 여러 변수 값을 하나의 선으로 이어나가 군집 경향을 파악합니다. 비정형 데이터 탐색도 이루어지는데, 예를 들어 텍스트 데이터의 탐색으로 워드클라우드를 만들어 자주 등장하는 단어를 시각화하거나, 토픽 모델링을 통해 자주 나타나는 주제를 파악합니다. 이미지나 그래프 데이터의 경우 차원축소 기법으로 2D플롯에 투영하여 대략적 군집 형태를 살펴보기도 합니다. 이러한 고급 탐색을 통해 데이터에 숨은 복잡한 구조나 특성을 발견하고, 분석 방향을 정교화할 수 있습니다.
통계 기법의 이해: 데이터 분석의 기반으로 통계학적 이론을 이해하고 있어야 합니다. 빅데이터분석기사 시험에서도 기술통계와 추론통계의 개념이 출제되며, 이는 분석 결과를 해석하고 모델을 검증하는 데 필수적입니다:
기술통계 (Descriptive Statistics): 앞서 언급한 대로 데이터의 요약과 설명을 다룹니다. 주요 개념으로 **분포(distribution)**와 특성치가 있습니다. 분포는 데이터의 퍼져있는 형태를 말하며, 정규분포, 균등분포, 이항분포 등 다양한 확률분포 모델이 있습니다. 예를 들어 키나 혈압 등은 대체로 정규분포를 따르고, 성공/실패 같은 이산 사건 횟수는 이항분포로 모델링합니다. 표본추출(sampling) 개념도 중요한데, 모집단 전체를 분석하기 어려울 때 대표 표본을 추출하여 분석하고 추론합니다​.
단순무작위추출, 층화추출, 군집추출 등 방법이 있으며, 표본이 대표성을 가져야만 결과를 모집단에 일반화할 수 있습니다. 추출된 표본 데이터의 통계량(표본평균 등)은 추출할 때마다 변동하므로, **표본분포(sampling distribution)**라는 개념으로 그 분포를 이론적으로 분석합니다​.
대표적으로 중앙극한정리에 따라 표본크기가 충분히 크면 표본평균은 정규분포에 가까워지는데, 이를 활용해 신뢰구간 등을 구할 수 있습니다.
추론통계 (Inferential Statistics): 표본에서 얻은 정보를 토대로 모집단의 특성을 추정하거나 가설을 검정하는 기법입니다. **점추정(point estimation)**은 어떤 모수(parameter)의 값을 하나의 통계량으로 추정하는 것이고, **구간추정(interval estimation)**은 추정치의 신뢰도를 고려해 **신뢰구간(confidence interval)**을 제시하는 것입니다. 예를 들어 표본 평균이 50이라면 모평균에 대해 95% 신뢰구간이 45~55와 같이 계산될 수 있습니다. **가설검정(hypothesis testing)**은 두 집단 평균 비교나 효과 검증 등에 활용하는데, 먼저 영가설(H0: 차이가 없다 등)과 대립가설(H1)을 세우고 표본 통계량으로부터 p-값을 계산하여 유의수준 하에서 영가설 기각 여부를 판단합니다. 흔히 사용하는 통계 검정으로 t-검정(t-test), 카이제곱 검정, 분산분석(ANOVA) 등이 있으며, 각각 평균의 차이 검정, 분할표에서 범주 간 독립성 검정, 세 집단 이상 평균 비교 등에 쓰입니다. 이러한 통계 기법은 모델링 결과를 평가하거나 특성을 해석할 때도 등장합니다. 예를 들어 회귀분석에서 회귀계수의 유의성 검정도 t-검정의 일종이고, 분류모델 평가에서도 카이제곱 통계를 사용할 수 있습니다.
이처럼 통계적 기초를 알고 있으면 데이터 분포 가정에 어긋나지 않게 모델을 선택하고, 신뢰도 있는 결론을 이끌어내는 데 도움됩니다. (만약 모집단 분포 가정이 어려운 경우에는 뒤에서 설명할 비모수 통계를 쓰는 방법도 있습니다​.) 
분석 모형 설계: 이제 탐색과 기획을 거쳐 적합한 분석 기법을 적용할 차례입니다. 먼저 풀고자 하는 문제에 맞는 **분석 모형(model)**을 선정하고 설계해야 합니다. 분석모형 선정이란 분류냐 회귀냐, 지도학습이냐 비지도학습이냐 등 분석 과제 유형에 맞는 모델 타입을 결정하는 것입니다. 예를 들어 고객 이탈 여부 예측은 분류(classification) 문제이므로 의사결정나무나 로지스틱 회귀 같은 분류 모형을 고려하고, 판매량 예측은 회귀(regression) 문제이므로 선형회귀나 시계열모델을 고려합니다. 혹은 군집이나 연관규칙 탐색처럼 비지도학습이 적합한 과제도 있습니다. 다음으로 선정한 모델 유형 내에서 구체적인 알고리즘(예: 결정트리 vs 랜덤포레스트, ARIMA vs LSTM 등)을 정합니다. 이때 데이터의 특성(정형/비정형, 변수 수, 레코드 수 등), 요구되는 설명력 vs 정확도, 해석 용이성, 계산 자원 등을 종합적으로 고려합니다. 예컨대 변수 수가 매우 많다면 차원축소 결합한 모델을, 해석력이 중요하면 결정트리 기반 모델을 선택하는 식입니다. 분석모형 정의 단계에서는 모델의 입력과 출력, 가정 사항을 명확히 규정합니다. 회귀모델이라면 독립변수와 종속변수를 정의하고 선형관계를 가정하는지 여부 등을 명시합니다. 분석 절차 및 환경 구축: 모델링을 수행하기 전에 분석 환경을 준비합니다. 어떤 분석 도구와 프로그래밍 언어를 사용할지 결정하고, 라이브러리나 패키지를 세팅합니다. 예를 들어 Python의 scikit-learn이나 R의 caret 패키지, 또는 TensorFlow 같은 딥러닝 프레임워크를 선택할 수 있습니다. 또한 하드웨어 환경(로컬 PC vs 분산 환경 vs 클라우드)을 정하고, 필요하다면 하둡이나 스파크 클러스터, GPU 서버 등을 구성합니다. 그 다음, 데이터 분할(data splitting) 작업을 합니다. 지도학습 모델의 경우 성능 평가와 과적합 방지를 위해 **훈련용 데이터(train set)**와 **테스트용 데이터(test set)**로 데이터셋을 나눕니다. 일반적으로 전체의 60~80%를 훈련에 사용하고, 나머지를 테스트에 사용하거나, 교차검증 기법을 활용하기도 합니다. 필요하면 별도의 검증용(validation) 데이터를 두어 하이퍼파라미터 튜닝에 쓰기도 합니다. 이처럼 환경을 갖추고 데이터를 준비하면, 설계한 모형에 따라 모델링(modeling) 단계로 넘어가게 됩니다. 주요 분석 기법 (머신러닝/통계 기법): 빅데이터분석기사에서는 여러 **분석 기법(알고리즘)**의 개념을 다룹니다. 여기서는 대표적인 방법들을 요약합니다:
회귀분석 (Regression Analysis): 연속형 숫자 값을 예측하기 위한 지도학습 기법입니다. 가장 기본이 되는 선형 회귀는 독립변수 𝑥와 종속변수 y 사이의 관계를 나타내는 선형 함수를 학습합니다. 
즉  𝑦 =𝑤𝑥+𝑏
y=wx+b 형태의 직선을 데이터에 가장 잘 맞추도록 
w,b 계수를 찾는 것입니다​. 
다중회귀분석은 여러 개의 독립변수에 대해 평면 또는 초평면 형태로 확장합니다. 회귀분석은 모델이 단순하고 결과 해석이 용이하며, 예측하고자 하는 현상의 추세와 영향 요인을 파악하는데 널리 사용됩니다. 하지만 관계가 비선형이거나 변수 간 상호작용이 있는 경우는 선형 회귀로는 한계가 있어, 다항 회귀나 비선형 회귀로 확장하거나 다른 기법을 써야 합니다.
로지스틱 회귀 (Logistic Regression): 이름은 회귀지만 **범주형 결과(분류)**를 예측하는 데 사용되는 지도학습 모델입니다. 종속변수가 Yes/No처럼 둘 중 하나로 분류되는 이항분류 문제에 주로 쓰이며, 출력으로 0과 1 사이의 확률값을 생성하는 것이 특징입니다​.
선형 회귀 방정식의 출력에 시그모이드(sigmoid) 함수를 적용하여 S자형 곡선으로 변환함으로써, 어떤 레코드가 positive 클래스일 확률 
p를 추정하고 p>0.5이면 긍정으로 분류하는 식입니다. 로지스틱 회귀는 해석이 쉽고 (각 변수의 회귀계수가 그 변수의 영향력을 의미) 계산이 빠르며 확률적 출력을 제공하여, 의료나 금융 등에서 이벤트 발생 확률 예측에 널리 활용됩니다. 다만 결정경계가 선형이기 때문에 복잡한 비선형 분류 문제에서는 성능이 낮을 수 있습니다.
의사결정나무 (Decision Tree): 자료를 트리 구조로 분류하거나 예측하는 기법입니다. 분할 규칙에 따라 데이터를 yes/no로 가지를 나누어가며 최종 잎노드에서 예측값을 산출합니다. 예를 들어 대출 심사에서 소득 수준 기준으로 한 번 분기하고, 부채율 기준으로 다시 분기하여 마지막 노드에서 "상환 불이행 위험 높음/낮음"을 분류하는 식입니다. 결정나무의 장점은 결과 해석이 직관적이라는 것입니다. 트리 구조 자체가 규칙의 집합이므로, 어떻게 예측에 이르렀는지 설명하기 쉽습니다. 또한 숫자형, 범주형 변수 모두 다룰 수 있고 전처리(스케일링 등)가 거의 필요없으며, 변수 간 비선형 관계도 자연스럽게 처리됩니다​. 하지만 트리가 너무 깊어지면 **과적합(overfitting)**되기 쉽고, 작은 데이터 변화에도 구조가 크게 바뀌는 불안정성이 있습니다​
. 이를 보완하기 위해 **가지치기(pruning)**를 하여 적절한 깊이를 유지하고, 랜덤포레스트 같은 앙상블 방법을 사용하기도 합니다.
인공신경망 (Artificial Neural Network): 뇌의 뉴런 구조를 모방한 복잡한 함수망을 통해 데이터의 패턴을 학습하는 기법입니다. 입력층-은닉층-출력층으로 이루어진 **여러 층(layer)**의 노드들이 가중치로 연결되어, 입력에서 출력까지의 변환을 수행합니다. 신경망은 선형 모델로는 포착하기 어려운 매우 복잡한 비선형 관계도 학습할 수 있으며, 충분한 데이터가 있으면 높은 예측 정확도를 보입니다. 특히 여러 은닉층을 쌓은 심층신경망(DNN) 구조는 **딥러닝(deep learning)**이라고 불리며 음성 인식, 이미지 분류, 자연어 처리 등에서 혁신적 성능 향상을 이끌었습니다. 예컨대 **합성곱 신경망(CNN)**은 이미지나 영상에서 특징을 추출하는데 탁월하고, **순환 신경망(RNN)**은 시계열 데이터나 언어 데이터처럼 순차적 특성이 있는 입력을 잘 처리합니다. 다만 신경망 모델은 매개변수(가중치)가 매우 많고 학습에 시간이 오래 걸리며, 결과를 해석하기 어려운 블랙박스 모델이라는 단점이 있습니다. 또한 과적합을 막기 위해 정규화, 드롭아웃(dropout) 등의 기법과 많은 데이터를 필요로 합니다. 그럼에도 최근의 빅데이터 환경에서는 풍부한 데이터와 계산 자원을 바탕으로 딥러닝 모델이 적극 활용되고 있습니다.
서포트 벡터 머신 (SVM): 주어진 데이터들을 분류하기 위해 **최적의 결정경계(hyperplane)**를 찾는 지도학습 기법입니다. 2차원 데이터를 예로 들면 두 클래스를 가장 여유 있게 갈라놓는 직선을 찾는 것과 같습니다. SVM은 **마진(maximum margin)**이라는 개념으로 학습하는데, 두 클래스 간 경계로부터 가장 가까운 점들(서포트 벡터)까지의 거리가 최대가 되는 선을 결정경계로 삼습니다. 이렇게 하면 일반화 능력이 높아진다고 알려져 있습니다. 선형으로 분리가 어려운 경우 **커널 트릭(kernel)**을 사용하여 데이터 공간을 고차원으로 매핑, 비선형 경계도 학습할 수 있습니다. 예를 들어 RBF 커널을 쓰면 곡선형태의 분리도 가능합니다. SVM은 이론적으로 탄탄하고 소규모 데이터에도 강건하며, 분류뿐 아니라 **회귀(SVR)**에도 응용됩니다. 그러나 데이터 건수가 매우 많거나 차원이 엄청 높을 때는 계산량이 많아 비효율적일 수 있고, 다중 클래스 문제를 직접 다루지는 못해 이진분류를 여러 번 조합해야 하는 단점이 있습니다. 최근엔 딥러닝에 밀렸지만, 중간 규모 데이터에선 여전히 높은 성능을 보이는 모델입니다.
연관 분석 (Association Analysis): 데이터 내 항목들 사이의 흥미로운 연관 규칙을 발견하는 비지도학습 기법입니다. 장바구니 분석으로 유명한 연관규칙학습은 *"빵을 산 사람은 우유도 살 확률이 높다"*와 같은 항목 간 동시 출현 패턴을 찾는 것이 주 목적입니다. 이는 **지원도(support)**와 신뢰도(confidence) 등의 지표를 통해 측정됩니다. 예를 들어 전체 거래 중 5%에서 빵과 우유를 함께 샀다면 support=0.05, 빵 산 거래의 30%에서 우유도 샀다면 confidence=0.3 식입니다. 대표적인 알고리즘으로 Apriori 알고리즘과 FP-Growth 등이 있으며, 다수의 아이템 집합을 효율적으로 탐색하여 높은 지지도와 신뢰도의 규칙들을 찾아냅니다. 연관 분석은 상품 진열 전략이나 추천 시스템(어떤 영화를 본 사람이 다른 어떤 영화를 볼 것인가 등)에서 활용되며, 데이터 마이닝의 한 분야로 취급됩니다. 대용량 거래 데이터에서도 비교적 빠르게 규칙 탐사가 가능하나, 유의미한 규칙을 해석하고 노이즈 규칙을 걸러내는 것은 분석가의 몫입니다.
군집분석 (Clustering): 레이블이 없는 데이터들을 특성이 비슷한 것끼리 그룹으로 묶는 비지도학습 기법입니다. 데이터 포인트들 사이의 유사도/거리를 계산하여, 서로 가까운 것들은 같은 클러스터로, 먼 것들은 다른 클러스터로 분류합니다. 가장 널리 쓰이는 **K-평균 군집(K-means)**은 찾고자 하는 군집 개수 K를 정하고, 무작위 초기 중심에서 시작해 각 점을 가장 가까운 중심에 할당 → 중심 재계산을 반복하여 수렴시키는 알고리즘입니다. 계산이 간단하고 빠르지만, K값을 미리 알아야 하고 구형(cluster shape)이 아닌 패턴에는 성능이 떨어질 수 있습니다. **계층적 군집분석(hierarchical clustering)**은 거리 행렬을 기반으로 가장 가까운 두 점/군집부터 순차적으로 합쳐나가 계층적 트리를 형성하는 방법(병합적 방법) 등이 있습니다. 결과는 덴드로그램으로 표시되어 군집을 자르는 기준선에 따라 다양한 군집수로 볼 수 있습니다. 그 밖에 밀도 기반의 DBSCAN 알고리즘 등도 있어, 밀도가 높은 영역을 클러스터로 식별하고 잡음 데이터는 아웃라이어로 구별할 수도 있습니다. 군집분석 결과는 고객 세분화, 이미지 세그멘테이션 등 많은 분야에서 활용되며, 지도학습 전 준비 단계로서 데이터의 잠재 구조를 파악하는 데에도 쓰입니다.
범주형 자료 분석: 데이터 중 범주형(categorical) 변수로 이루어진 자료의 특성을 분석하는 기법들을 말합니다. 범주형 자료는 값이 몇 가지 범주로 구분되는 데이터로, 성별, 혈액형, 지역, 제품등급 등이 해당됩니다. 이들의 분포와 관계를 파악하기 위해 교차표 분석과 **카이제곱 검정(χ² test)**을 자주 사용합니다. 예를 들어 성별에 따라 선호 제품 종류에 차이가 있는지 알아보고 싶다면, 성별×제품 범주의 교차표를 만들고 카이제곱 독립성 검정을 통해 두 변수의 독립/관련 여부를 판단합니다. 또한 범주형 자료의 특성을 모델링하는 로지스틱 회귀(앞서 설명)나 로지트(logit)/프로빗 회귀 등이 범주형 종속변수의 확률을 다루는 통계 모델입니다. 분할표 분석을 통해 잔차를 살펴보거나, 이항 로지스틱을 다중 범주로 확장한 다항 로지스틱 회귀 등도 범주형 데이터 분석에 속합니다. 시험 대비로서는 카이제곱 검정의 해석(기대도수와 관찰도수, p-value)이나 분할표에서의 실제 빈도 vs 기대 빈도 비교 등을 알고 있어야 합니다.
다변량 분석: 여러 변수들을 동시에 고려하여 숨은 구조나 관계를 분석하는 방법들을 통칭합니다. 차원축소 기법인 **주성분 분석(PCA)**도 다변량 분석의 하나이고, 군집분석도 사실 다변량 기법입니다. 여기서는 주로 다변량 통계 모형을 지칭하는데, 대표적으로 **판별분석(Discriminant Analysis)**과 군집을 제외한 요인분석(Factor Analysis) 등이 있습니다. 판별분석은 주어진 데이터로부터 미리 정의된 그룹(클래스)을 잘 구분할 수 있는 선형 판별함수를 찾는 기법으로, 머신러닝의 분류와 유사하지만 통계적 가정(등분산 가정 등)이 있는 방법입니다. 요인분석은 여러 관측변수들의 상관관계를 분석하여, 그 배후에 존재하는 숨은 공통인자(요인)를 도출하는 기법입니다. 예를 들어 여러 소비자 행동 변수들에 대해 요인분석을 실행하면 "충동 구매 성향", "가격 민감도" 같은 요인이 추출될 수 있고, 이는 각 변수들을 몇 개의 잠재 차원으로 요약해 해석하는 데 도움이 됩니다. 또한 주성분 분석 역시 다변량 데이터에서 상관된 변수들을 요약 정리하는 데 쓰이며, **공변량 구조분석(Structural Equation Modeling, SEM)**처럼 복수의 회귀식을 동시에 추정하여 인과모형을 검증하는 고급 기법도 다변량 분석에 포함됩니다. 시험에서는 PCA 개념이나 요인분석의 목적 등을 묻거나, 공분산-분산 행렬 개념 등을 다룰 수 있습니다.
시계열 분석 (Time Series Analysis): 시간의 흐름에 따라 순서대로 관측된 데이터(시계열 데이터)를 다루는 분석 기법입니다. 경제 지표, 주가, 기후 데이터, 센서 로그 등 시간순 데이터에는 시간적 자기상관과 추세/계절성이 존재할 수 있기 때문에, 일반적인 회귀와는 달리 시계열 전용 모델을 사용합니다. 가장 전통적인 시계열 모델은 ARIMA 모델입니다. ARIMA는 자기회귀(AR) 항과 차분(I), 이동평균(MA) 항을 조합한 모델로, 지난 
p시점의 값과 q시점까지의 예측오차를 선형 결합하여 현재값을 설명합니다. 이 모델은 Stationary한 시계열(평균과 분산이 시간에 따라 일정한 데이터)에 적용하며, 비정상 시계열은 차분을 통해 정상화한 후 적용합니다. 계절 효과가 있으면 계절 차분을 추가한 SARIMA 모델을 사용합니다. ARIMA 모형의 모수 추정과 모형 선택은 ACF/PACF 그래프나 정보기준(AIC, BIC)을 활용합니다. 이밖에 추세와 계절성을 비모수적으로 분해하는 STS(분해시계열) 기법, 또는 **지수평활법(Exponential Smoothing)**을 통한 예측 등도 있습니다. 최근에는 LSTM 같은 딥러닝 순환신경망이 시계열 예측에 많이 쓰이고 있지만, 시험 범위로는 Box-Jenkins의 ARIMA 모델 정도가 중요합니다. 시계열 분석에서는 자기상관함수(ACF), 부분자기상관함수(PACF), 정상성 등의 용어와, 예측 성능 지표(RMSE 등)를 알아두어야 합니다.
베이지안 기법 (Bayesian Methods): 베이지안 통계는 베이즈 정리를 활용하여 사전 지식(사전분포)과 데이터 증거를 결합해 사후분포를 얻는 접근입니다. 전통 빈도주의 통계와 달리 모수 자체를 확률변수로 취급해, 데이터가 관측된 후 모수가 특정 값일 확률분포를 계산할 수 있습니다. 베이지안 방법의 이론적 상세는 복잡하지만, 개념적으로는 **기존의 믿음(prior)**을 가지고 있다가 데이터를 통해 이를 **업데이트(update)**한다는 점에서, 추가적인 정보가 누적되어도 쉽게 추론을 갱신할 수 있는 장점이 있습니다. 머신러닝에서 유명한 나이브 베이즈 분류는 베이즈 정리를 활용한 간단한 분류기인데, 독립 가정 하에 각 클래스의 사전확률과 클래스별 특성들의 가능도를 계산하여 가장 높은 사후확률을 가진 클래스로 분류합니다. 비록 특성 독립이라는 강한 가정 때문에 "나이브"하지만 텍스트 분류 등에서 성능이 준수하고 계산이 빠릅니다. 그밖에 베이지안 네트워크(확률 그래픽 모델)나 MCMC 기법을 이용한 베이지안 추정 등 고급 주제도 있지만, 시험에서는 베이즈 정리와 나이브베이즈 정도가 주로 다뤄질 것입니다. 핵심은 사전확률 × 우도 = 사후확률의 맥락을 이해하는 것입니다.
딥러닝 분석: 앞서 신경망에서 일부 다뤘지만, 딥러닝은 현대 빅데이터 분석의 핵심 분야이므로 추가로 언급합니다. **딥러닝(Deep Learning)**은 수백만 개 이상의 가중치를 가진 다층 신경망을 대량의 데이터로 학습시켜 복잡한 예측이나 패턴 인식을 가능케 합니다. 인공지능 열풍으로 인해 이미지 분류(예: CNN으로 개와 고양이 구별), 음성 인식(시계열 신경망으로 음성→텍스트 변환), 기계 번역(seq2seq 모델로 언어 간 번역) 등 다양한 응용이 발전했습니다. 빅데이터는 딥러닝의 필수 요소인 풍부한 훈련 데이터를 제공하고, GPU 같은 하드웨어의 발전과 맞물려 딥러닝 모델의 성능이 큰 향상을 이루었습니다. 딥러닝 모델 학습에는 **역전파 알고리즘(backpropagation)**으로 가중치를 조정하며, 과적합을 막기 위해 드롭아웃, 정규화, 조기 종료 같은 기법을 동원합니다. 딥러닝은 높은 정확도를 얻을 수 있지만 모델 해석이 어려운 것이 단점입니다. 
최근에는 SHAP 값이나 LIME 등 설명 가능한 AI(XAI) 기법으로 딥러닝의 의사결정 근거를 부분적으로나마 설명하려는 시도도 나타나고 있습니다. 빅데이터 분석 전문가라면 완전히 자동화된 딥러닝 모델의 결과도 논리적으로 검증하고 비즈니스 의미를 해석할 수 있어야 할 것입니다.
비정형 데이터 분석: 비정형 데이터(텍스트, 이미지, 영상, 음성 등)를 분석하는 기법들입니다. 텍스트 분석에서는 자연어 처리(NLP) 기술을 활용하여 텍스트를 정제(tokenization, 불용어 제거 등)하고, 문서-단어 행렬이나 워드 임베딩으로 수치화하여 분석합니다. 감성 분석(sentiment analysis)처럼 문장의 긍정/부정을 분류하거나, 토픽 모델링(LDA 기법)으로 문서 집합에서 주제들을 추출하는 것이 흔한 과제입니다. 이미지 분석은 주로 딥러닝의 CNN을 활용하며, 사전에 이미지넷 같은 대형 데이터셋으로 학습된 사전학습 모델을 쓰거나 전이학습으로 특성을 추출한 후 분류/클러스터링을 합니다. 영상 데이터는 CNN+RNN 조합으로 처리하거나 3D CNN을 쓰기도 합니다. 소셜 네트워크 데이터 같은 그래프 데이터는 노드와 엣지로 표현하여 연결 중심성, 커뮤니티 발견 등의 네트워크 분석을 수행합니다. 이러한 비정형 데이터 분석은 고도의 전문성이 필요하지만, 빅데이터 범위 확대 측면에서 중요성이 커지고 있습니다. 시험에서는 비정형 데이터의 개념적 처리 과정 (예: 텍스트 → 자연어처리 → 감성분석)이나, 정형 vs 비정형의 차이점 등을 물을 수 있습니다.
앙상블 분석 (Ensemble Analysis): 여러 모델을 조합하여 더 나은 예측 성능을 얻는 기법입니다. 하나의 모델보다 다수 모델의 결합이 오히려 성능이 향상되는 경우가 많다는 아이디어에 기반합니다​. 분류 문제의 경우 여러 분류기의 예측 결과 중 다수결 투표를 통해 최종 클래스를 결정하거나, 회귀 문제는 평균값을 결합 결과로 삼습니다​
. 앙상블 학습은 개별 모델들이 서로 다른 유형의 오차를 가질 때 효과적입니다. 결정트리, SVM, 로지스틱 회귀처럼 성격이 다른 모델들을 함께 사용할 수도 있고, 또는 동일 모델의 훈련 데이터 샘플링 변형을 통해 여러 분류기를 만들 수도 있습니다. 대표적인 앙상블 기법으로 **배깅(Bagging)**과 **부스팅(Boosting)**이 있습니다. 배깅은 Bootstrap Aggregating의 약자로, 원본 데이터에서 부트스트랩 샘플(중복을 허용한 랜덤샘플)을 여러 개 뽑아 각각 모델을 학습시킨 후 결과를 평균/투표로 결합하는 방식입니다​
. **랜덤 포레스트(Random Forest)**는 배깅의 일종으로, 결정트리를 다수 생성하면서 학습 시 랜덤한 변수 선택까지 적용하여 트리들 간 상관성을 낮춘 앙상블입니다 (이를 통해 과적합을 줄이고 성능을 높임). 부스팅은 약한 모델들을 순차적으로 학습시키되, 이전 모델이 틀린 부분을 다음 모델이 보완하도록 가중치를 조정하면서 학습하는 방식입니다. 가장 유명한 것은 AdaBoost와 그 발전형인 Gradient Boosting(XGBoost, LightGBM 등)으로, 전자의 경우 이전 모델이 misclassify한 데이터에 가중치를 더 주어 다음 모델이 학습하게 하고, 후자의 경우 잔여 오차(residual)를 다음 모델이 예측하도록 유도합니다. 앙상블은 이처럼 **동일 알고리즘 반복 배치(배깅)**나 순차 보완(부스팅) 뿐만 아니라, **스태킹(Stacking)**이라 해서 서로 다른 알고리즘의 예측을 메타 모델이 다시 학습하는 방법도 있습니다. 전반적으로 앙상블 기법들은 Kaggle 경진대회 등에서 입증되었듯이 예측 성능을 상당히 끌어올릴 수 있으며​
, 실제 산업 현장에서도 많이 활용되고 있습니다. 단점이라면 결과가 여러 모델의 복합이라 해석이 어려워진다는 것과, 속도가 느릴 수 있다는 점입니다.
비모수 통계: 끝으로, 통계 분석에서 모수 분포 가정에 의존하지 않는 방법들을 가리킵니다. 많은 통계 기법들이 데이터가 정규분포를 따른다거나 분산이 일정하다는 등의 가정을 전제로 하지만, 비모수(non-parametric) 방법은 이러한 가정을 최소화하고 데이터 자체로부터 추론을 합니다​
. 예를 들어 정규성 가정을 할 수 없을 때 평균 대신 순위(rank) 개념으로 두 집단을 비교하는 윌코xon 순위합 검정이나 망Whitney U 검정 같은 것이 비모수 검정입니다. 중앙값에 대한 부호검정이나, 세 집단 이상에 대한 크러스칼-왈리스 검정 등도 사용됩니다. 비모수 방법은 표본 크기가 작거나 분포가 너무 특이한 경우 유용하며, 이상치의 영향도 적은 편입니다​
. 다만 모수적 방법에 비해 검정력이 떨어지거나, 큰 데이터에서는 계산이 복잡해지는 단점도 있습니다​
. 시험에서는 비모수 통계의 정의(“모집단 분포에 대한 가정 없이 데이터 분석을 하는 방법”​
)나 언제 비모수 방법을 쓰는지(정규성 만족 못할 때 등)를 묻거나, 대표적인 비모수 검정 이름을 물을 수 있습니다. 분석 실무에서도 정규분포 가정이 안 맞으면 비모수적 방법을 고려해야 하므로 개념을 알아두어야 합니다.
위에 열거한 기법들은 빅데이터 분석에서 빈출되는 주요 알고리즘과 통계 방법들로, 각 기법의 원리, 적용조건, 장단점 등을 이해해야 합니다. 시험에서는 이들의 개념적 설명이나 비교, 또는 간단한 계산/해석 문제가 출제될 수 있습니다. 모형 평가 및 검증: 모델을 만들었다면, 이를 제대로 평가하고 개선하는 것이 다음 과제입니다. 분석 모형 평가는 모델의 예측 성능과 적합도를 측정하는 단계입니다. 먼저 **평가 지표(metric)**를 적절히 선택합니다. 회귀 문제에서는 MAE(Mean Absolute Error), MSE(Mean Squared Error), RMSE, R² (결정계수) 등이 쓰이고, 분류 문제에서는 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-score, ROC-AUC 등이 사용됩니다. 예를 들어 불균형 데이터에서는 Accuracy보다는 Precision/Recall이나 AUC가 더 의미있는 지표가 될 수 있습니다. 평가 지표에 따라 모형을 진단합니다. 훈련 데이터에 대해서는 성능이 높지만 테스트 데이터에 낮다면 **과적합(overfitting)**을 의심할 수 있습니다. 반대로 훈련 데이터 성능 자체가 낮다면 **미적합(underfitting)**으로 모델이 복잡한 패턴을 학습하지 못한 것입니다. 회귀 모델에서는 잔차(실제값-예측값) 분석을 통해 모형의 가정 (선형성, 등분산성, 정규성)이 만족되는지 진단합니다. 잔차에 특이한 패턴이 없다면 모델이 데이터를 잘 표현한 것이지만, 잔차에 추세나 분산 변화가 보이면 모형 가정을 위배한 것이므로 모형을 재고해야 합니다. 또한 **교차 검증(cross-validation)**을 활용하여 모형의 안정적 성능을 확인합니다. K-겹 교차검증은 데이터를 K등분하여 그 중 1개를 테스트로, 나머지를 훈련으로 사용하여 평가를 K번 반복하는 방식으로, 데이터 분할에 따른 성능 편차를 줄여줍니다. 교차검증 결과 평균 성능이 만족스럽다면 일반화 성능이 확보된 것으로 볼 수 있습니다. 모수 유의성 검정: 회귀분석 등 통계모형에서는 추정된 모수(회귀계수 등)가 유의미한지 검정하는 절차가 있습니다. 예를 들어 단순회귀에서 기울기 
β 
1 에 대한 t-검정을 통해 이 값이 0이 아닌지 판단합니다. p-값이 유의수준 0.05보다 작으면 해당 변수가 통계적으로 유의한 영향을 미친다고 보고, 높으면 영향이 불확실하다고 판단합니다. 다중회귀의 F-검정은 전체 모형이 유의한지를 검사합니다. 이러한 모형 적합도 검정(goodness-of-fit) 결과를 통해 모델을 개선할 힌트를 얻습니다. 또한 분류모델에서는 **혼동행렬(confusion matrix)**을 통해 어떤 유형의 오류(False Positive/Negative)가 많은지 파악하고, ROC 곡선으로 다양한 임곗값에서 민감도-특이도를 살펴 모델을 평가합니다. 분석모형 개선: 평가 결과를 토대로 모델을 개선하는 전략을 적용합니다. 가장 먼저 과대적합 방지를 위해 복잡도를 조절합니다. 의사결정나무라면 가지치기로 트리 깊이를 제한하고, 선형모델이나 신경망이라면 정규화(regularization) 항을 추가하여 과도한 가중치 값을 억제합니다. Ridge, Lasso 회귀 등은 이런 규제 기법을 사용한 예입니다. 또한 더 많은 학습 데이터를 확보하거나 교차검증으로 튜닝하여 일반화 능력을 높입니다. 매개변수 최적화 (하이퍼파라미터 튜닝): 모델 자체의 설정값(나무의 개수, 학습률, 신경망의 은닉노드 수 등)을 조정하여 성능을 높입니다. 그리드 서치(Grid Search)나 랜덤 서치, 베이지안 최적화 같은 기법으로 여러 파라미터 조합을 시험해보고 교차검증 점수가 가장 좋은 모델을 선택합니다. 예컨대 SVM의 경우 RBF 커널일 때 **감마(gamma)**와 정규화 파라미터 C 값을 최적화해야 하고, 랜덤포레스트는 트리 개수와 최대 깊이 등을 조정해야 합니다. 딥러닝에서는 학습률, batch size, epoch 수 등을 튜닝합니다. 이처럼 하이퍼파라미터 튜닝을 통해 모델 성능을 미세 조정할 수 있습니다. 모형 융합(앙상블): 앞서 설명한 앙상블 방법을 통해 여러 모델을 결합하여 성능 향상을 꾀합니다. 예를 들어 개별적으로 훈련된 로지스틱 회귀, 결정나무, SVM 모델이 있다면, 이들의 예측결과를 취합하여 다수결로 최종 분류하는 보팅(voting) 분류기를 만들 수 있습니다. 또는 서로 다른 모델들의 예측값을 새로운 데이터로 삼아 메타 모델(예: 간단한 회귀나 로지스틱)을 학습시키는 **스태킹(stacking)**도 가능합니다. 이렇게 하면 개별 모델이 놓치는 부분을 다른 모델이 보완하여 종합 성능을 높일 수 있습니다. 다만 앙상블은 복잡도가 올라가므로, 단일 모델로도 충분한 경우 굳이 사용할 필요는 없습니다. 최종 모형 선정: 여러 후보 모델 중에서 실제 적용할 최종 모델을 결정합니다. 최종 선정은 단순히 성능 수치만 보는 것이 아니라, 모델의 복잡도, 해석 가능성, 실행 시간, 유지보수 용이성 등을 종합적으로 고려합니다. 예를 들어 성능 차이가 미미하다면 더 단순한 모델을 채택하는 편이 좋습니다 (오캄의 면도날 원칙). 또한 현업에 적용 시 실시간으로 예측 가능한지, 특별한 인프라가 필요한지 등의 실용성도 검토합니다. 최종 선택된 모델은 테스트 데이터나 별도의 검증 데이터에서 충분히 만족스러운 성능을 보여야 하며, 이해관계자에게 결과를 설명할 수 있을 정도로 신뢰성이 확보되어야 합니다. 이러한 평가 및 개선 단계를 거쳐 신뢰할 수 있는 분석 모델이 완성됩니다. 이제 이 모델의 결과를 해석하고 활용하는 단계만 남게 됩니다.