참고Clickhereto download the full example code준비 운동: NumPy¶\(y=\sin(x)\)을 예측할 수 있도록,\(-\pi\)부터\(pi\)까지
유클리드 거리(Euclidean distance)를 최소화하도록 3차 다항식을 학습합니다.이 구현은 NumPy를 사용하여 순전파 단계와 손실(loss), 역전파 단계를 직접 계산합니다.NumPy 배열은 일반적인 n-차원 배열로, 딥러닝이나 변화도(gradient), 연산 그래프(computational graph)는
알지 못하며 일반적인 수치 연산을 수행합니다.99 295.370363953454
199 206.5152652889775
299 145.35828021925613
399 103.21874722291226
499 74.15133890715553
599 54.07946012922541
699 40.204677403572006
799 30.603790376475178
899 23.953610718049816
999 19.34275569134408
1099 16.142797536542922
1199 13.91994795008097
1299 12.374456482491272
1399 11.298978792054733
1499 10.549944464052064
1599 10.027843305107437
1699 9.663637206426824
1799 9.40938404546334
1899 9.23176108841335
1999 9.107586565754804
Result: y = 0.016672704857992902 + 0.8504174875880517 x + -0.0028763190230101402 x^2 + -0.09243094678701738 x^3importnumpyasnpimportmath# 무작위로 입력과 출력 데이터를 생성합니다x=np.linspace(-math.pi,math.pi,2000)y=np.sin(x)# 무작위로 가중치를 초기화합니다a=np.random.randn()b=np.random.randn()c=np.random.randn()d=np.random.randn()learning_rate=1e-6fortinrange(2000):# 순전파 단계: 예측값 y를 계산합니다# y = a + b x + c x^2 + d x^3y_pred=a+b*x+c*x**2+d*x**3# 손실(loss)을 계산하고 출력합니다loss=np.square(y_pred-y).sum()ift%100==99:print(t,loss)# 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.grad_y_pred=2.0*(y_pred-y)grad_a=grad_y_pred.sum()grad_b=(grad_y_pred*x).sum()grad_c=(grad_y_pred*x**2).sum()grad_d=(grad_y_pred*x**3).sum()# 가중치를 갱신합니다.a-=learning_rate*grad_ab-=learning_rate*grad_bc-=learning_rate*grad_cd-=learning_rate*grad_dprint(f'Result: y ={a}+{b}x +{c}x^2 +{d}x^3')Total running time of the script:( 0 minutes  0.133 seconds)DownloadPythonsourcecode:polynomial_numpy.pyDownloadJupyternotebook:polynomial_numpy.ipynbGallery generated by Sphinx-Gallery