from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
import json
import os
import random
from typing import Annotated, TypedDict, List, Literal
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage , SystemMessage
from langchain_teddynote.graphs import visualize_graph
from langchain_core.runnables import RunnableLambda, RunnableMap, RunnableConfig
from langchain_core.prompts import ChatPromptTemplate
import ast
from IPython.display import Image, display


# RAG ê´€ë ¨ ëª¨ë“ˆ ì¶”ê°€
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader, TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from typing import Annotated, List
from datetime import datetime
from langchain_core.output_parsers import PydanticOutputParser
from typing import Literal, get_args

# src ë‚´ ëª¨ë“ˆ import
import ModelManager
import PromptTemplate
import RagManager
import Structure

########################
### ë…¸ë“œ ì´ë¦„ ì •ì˜ ###
########################
NODE_VERIFY = 'Node_Verify'
NODE_GENERATE = 'Node_Generate'
NODE_PROMPT = 'Node_Prompt'
NODE_RAG = 'Node_RAG'
NODE_LOGIC_VERIFY = 'Node_Logic_Verify'
NODE_PROPER_VERIFY = 'Node_Proper_Verify'
NODE_SAVE_RESULT = 'Node_Save_Result'



def decide_attr_random(attr):
    attr_values = get_args( Structure.MultiChoiceQuestion.__annotations__[attr])
    return random.choice(attr_values)

    
# âœ… 1). RAG ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë…¸ë“œ
def retrieve_context(state):
    
    # """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë¬¸ë§¥ì„ ì¶”ê°€"""
    # if state.get("use_rag", False):
    return {"context": "", "use_rag": False ,
            'node_name' : NODE_RAG ,  'graph_flow' : (state.get('graph_flow', []) + [NODE_RAG]) }
    
    # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    query = f"{state['exam_name']} ì‹œí—˜ {difficulty} ë‚œì´ë„"
    
    # # RAG ë§¤ë‹ˆì €ë¥¼ í†µí•´ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    # rag_manager = RagManager.RAGManager()
    
    # try:
    #     # ì´ë¯¸ ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì—ˆë‹¤ê³  ê°€ì •
    #     docs = rag_manager.get_relevant_documents(query)
    #     context = rag_manager.get_context_str(docs)
    # except ValueError:
    #     # ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì´ˆê¸°í™” í›„ ê²€ìƒ‰
    #     rag_manager.load_and_process_documents()
    #     docs = rag_manager.get_relevant_documents(query)
    #     context = rag_manager.get_context_str(docs)
    
    # return {"context": context, "difficulty" : difficulty ,
    #         "use_rag": True , 'graph_flow' : (state.get('graph_flow', []) + [NODE_RAG]) , 'node_name' : NODE_RAG}



# âœ… 2). í”„ë¡¬í”„íŠ¸ ë…¸ë“œ
def prompt_node(state):
    
    
    print("?"*100)
    
    """RAG ì—¬ë¶€ì— ë”°ë¼ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©"""
    # í•„ìš”í•œ ë³€ìˆ˜ ì¶”ì¶œ
    exam_name = state.get("exam_name", "ADsP")
    topic = state.get("topic", "ë²”ìœ„ ì „ì²´")
    num_question = state.get('num_question', 3)
    difficulty = state.get("difficulty", "ì¤‘")
    use_rag = state.get("use_rag", False)
    context = state.get("context", "")

    parser = PydanticOutputParser(pydantic_object=Structure.MultiChoiceQuestion)
    

    # RAG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ (ì¶”í›„)
    messages = PromptTemplate.standard_prompt_template.format_messages(
        exam_name=exam_name,
        topic = topic,
        difficulty = difficulty,
        num_question=num_question,
        format=parser.get_format_instructions()
    )
        
    
    # ë©”ì‹œì§€ë¥¼ stateì— ì¶”ê°€
    return {"messages" : messages , "prompt_message": messages , 'graph_flow' : (state.get('graph_flow', []) + [NODE_PROMPT]) , 'node_name' : NODE_PROMPT }


def build_parallel_model_map(model_names):
    
    return RunnableMap({
    "messages": lambda state: state["messages"],
    "model_response": RunnableMap({
        model_name: RunnableLambda(lambda state, m=model_name: generate_response(state, m)) for model_name in model_names
    }),
    "exam_name": lambda state: state["exam_name"],
    "difficulty": lambda state: state["difficulty"],
    "num_question": lambda state: state["num_question"],
    "use_rag": lambda state: state["use_rag"],
    "context": lambda state: state["context"],
    'graph_flow' : lambda state : (state.get('graph_flow', []) + [NODE_GENERATE]) ,
    'node_name' : lambda state : 'Node_Generate'
})


# âœ… 3. LLM ëª¨ë¸ ì‘ë‹µ ìƒì„± ë…¸ë“œ (ë³‘ë ¬ ì²˜ë¦¬)
parallel_models = RunnableLambda(lambda state : build_parallel_model_map(state.get('models').keys()))


# ëª¨ë¸ë³„ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(state, model_name):

    # ëª¨ë¸ ë¡œë“œ
    model = state.get('models')[model_name]
    
    # LLM ì‘ë‹µ ìƒì„±
    response = model.invoke(state["messages"])
    
    print("##### í…ŒìŠ¤íŠ¸ ####")
    print(response)
    
    # ì¸í’‹/ì•„ì›ƒí’‹ í† í° ê°€ê²© ê³„ì‚°
    model_input_tokens = response.usage_metadata['input_tokens']
    model_output_tokens = response.usage_metadata['output_tokens']
    response_price = state.get('models_info')[model_name]['Input']*model_input_tokens + state.get('models_info')[model_name]['Output']*model_output_tokens

    # ë¬¸ì œ ê°ì²´í™”
    json_str = response.content
    str_question_lst = ast.literal_eval(json_str)

    # question_lst = [MultiChoiceQuestion(**json) for json in str_question_lst]

    print(str_question_lst)
    
    return {'content': response.content, 'cost': response_price , 'questions' : str_question_lst }


# 4ì§€ì„ ë‹¤í˜• ë¬¸ì œë“¤ ë¡œì§ ì²´í¬
def MultipleChoiceQuestion_logic_check(model , question):

    choices = ["choice_1" , "choice_2" , "choice_3" , "choice_4"]

    for choice in choices:
        model.invoke({'question' : question['question'] , 'choice' : choice})
    

# ê²€ì¦ ë…¸ë“œ
def logic_verify(state):

    Flag = True

    return {'node_name' : NODE_LOGIC_VERIFY , 'graph_flow' : (state.get('graph_flow') + [NODE_LOGIC_VERIFY]) , 'logic_test' : Flag}

# ë¡œì§ í…ŒìŠ¤íŠ¸ - ë¼ìš°íŒ…í•¨ìˆ˜
def logic_routing(state):
    if state.get('logic_test' , True):
        return NODE_PROPER_VERIFY
    else:
        return NODE_RAG

# ê²€ì¦ ë…¸ë“œ
def proper_verify(state):

    Flag = True
    return {'node_name' : NODE_PROPER_VERIFY , 'graph_flow' : (state.get('graph_flow') + [NODE_PROPER_VERIFY]) , 'proper_test' : Flag}


# ì í•© í…ŒìŠ¤íŠ¸ - ë¼ìš°íŒ…í•¨ìˆ˜
def proper_routing(state):
    if state.get('proper_test' , True):
        return NODE_SAVE_RESULT
    else:
        return NODE_RAG

# ìƒì„±í•œ ë¬¸ì œ ì €ì¥ ë° ì¶œë ¥
def save_result(state):

    state['graph_flow'] += [NODE_SAVE_RESULT]

    for model_name in state['model_response']:
        
        print(f"âœ… ë‹µë³€ ëª¨ë¸: {model_name}")
        print(f"âœ… í† í° ë°œìƒ ë¹„ìš©: {state['model_response'][model_name]['cost']}")

        questions = state['model_response'][model_name]['questions']
        print_questions_pretty(questions)
        
        datetime_str = datetime.now().strftime("%Y%m%d_%H%M")
        file_name = f"{generate_save_path}/{state['exam_name']}_{state['topic']}_{datetime_str}.json"
        
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(questions, f, ensure_ascii=False, indent=4)
       
        print(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {file_name}")    
        return {'node_name' : NODE_SAVE_RESULT} 


        
def print_questions_pretty(questions):
    
    print(f"ğŸ“˜ [{questions[0]['exam_name']}] - {len(questions)} ë¬¸í•­")
    print("-" * 60)
    for q in questions:  
        print(f" ì£¼ì œ: {q['topic']} / ë‚œì´ë„: {q['difficulty']}")
        print(f"ğŸ“ Q{q['num']}. {q['question']}")
        print(f"   A. {q['choice_1']}")
        print(f"   B. {q['choice_2']}")
        print(f"   C. {q['choice_3']}")
        print(f"   D. {q['choice_4']}\n")
        print(f"   í•´ì„¤ :  {q['answer']}")
        print("-" * 60)



def main_graph():

    # StateGraph ìƒì„±
    builder = StateGraph(State)

    ## ë…¸ë“œ ì„¤ì • ##
    builder.add_node( NODE_RAG, RunnableLambda(retrieve_context))       # RAG ê²€ìƒ‰
    builder.add_node( NODE_PROMPT , RunnableLambda(prompt_node))        # í”„ë¡¬í”„íŠ¸
    builder.add_node( NODE_GENERATE , parallel_models)                  # LLM ì‘ë‹µìƒì„±  
    builder.add_node( NODE_LOGIC_VERIFY , logic_verify)                 # ë…¼ë¦¬ê²€ì¦
    builder.add_node( NODE_PROPER_VERIFY , proper_verify)               # ì í•©ì„± ê²€ì¦ 
    builder.add_node( NODE_SAVE_RESULT , save_result)                   # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥

    ## ì—£ì§€ ì„¤ì • ##
    builder.set_entry_point(NODE_RAG)
    builder.add_edge(NODE_RAG, NODE_PROMPT)
    builder.add_edge(NODE_PROMPT, NODE_GENERATE)
    builder.add_edge(NODE_GENERATE, NODE_LOGIC_VERIFY)
    builder.add_edge(NODE_SAVE_RESULT, END)


    ## ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì • ##
    builder.add_conditional_edges( NODE_LOGIC_VERIFY, logic_routing ,
        { NODE_PROPER_VERIFY : NODE_PROPER_VERIFY ,   NODE_RAG: NODE_RAG }
    )

    ## ì¡°ê±´ë¶€ ì—£ì§€ ì„¤ì • ##
    builder.add_conditional_edges( NODE_PROPER_VERIFY, proper_routing ,
        { NODE_SAVE_RESULT : NODE_SAVE_RESULT ,  NODE_RAG : NODE_RAG }
    )

    # ê·¸ë˜í”„ ì»´íŒŒì¼
    app = builder.compile(checkpointer = MemorySaver())
    
    # visualize_graph(app)
    
    # LangGraph ì‹œê°í™” ì €ì¥
    img = Image(app.get_graph().draw_mermaid_png())

    with open("MainGraph.png", "wb") as f:
        f.write(img.data)

    return app




